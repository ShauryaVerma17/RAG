{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"llama3\"; \n",
    "Embedding_Model = \"mxbai-embed-large\"\n",
    "model = Ollama(model=Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1161, which is longer than the specified 1000\n",
      "Created a chunk of size 2033, which is longer than the specified 1000\n",
      "Created a chunk of size 2603, which is longer than the specified 1000\n",
      "Created a chunk of size 1782, which is longer than the specified 1000\n",
      "Created a chunk of size 5099, which is longer than the specified 1000\n",
      "Created a chunk of size 2110, which is longer than the specified 1000\n",
      "Created a chunk of size 1398, which is longer than the specified 1000\n",
      "Created a chunk of size 3672, which is longer than the specified 1000\n",
      "Created a chunk of size 1463, which is longer than the specified 1000\n",
      "Created a chunk of size 1654, which is longer than the specified 1000\n",
      "Created a chunk of size 2628, which is longer than the specified 1000\n",
      "Created a chunk of size 1706, which is longer than the specified 1000\n",
      "Created a chunk of size 1773, which is longer than the specified 1000\n",
      "Created a chunk of size 2232, which is longer than the specified 1000\n",
      "Created a chunk of size 2492, which is longer than the specified 1000\n",
      "Created a chunk of size 2454, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(path=\"testDataMD\",glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"##\", chunk_size=1000, chunk_overlap=0)\n",
    "docs2 = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = FAISS.from_documents(docs2, OllamaEmbeddings(model = Model))\n",
    "retriever2 = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to combine retrieved documents into a string \n",
    "\n",
    "def combineDocs(docs):\n",
    "    context = \"\\n\\n\".join(f'Document [ Metadata : \\n{doc.metadata} \\nPage Content : \\n{doc.page_content} ]' for doc in docs)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 (Complicated internet answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vectorDB):\n",
    "    \n",
    "    # This is where the conversation will be stored\n",
    "    memory = ConversationBufferMemory()\n",
    "\n",
    "    # Creating a conversation model for ConvChain\n",
    "\n",
    "    def rag_retrieval(question, memory):\n",
    "\n",
    "        # Retrieve docs from vector DB\n",
    "        docs = vectorDB.similarity_search(question, k = 4)\n",
    "        context = combineDocs(docs)\n",
    "\n",
    "        conversation = memory.load_memory_variables({}).get('history', '')\n",
    "\n",
    "        # Add context to the prompt\n",
    "        prompt = f'Answer the question based on the context below and the previous conversations. If you cannot\\nanswer the question, reply \"Oof that\\'s a tough one, i don\\'t really know this\"\\n\\nPrevious Conversation : \\n{conversation}\\n\\nContext : \\n{context}\\n\\nQuestion from user: {question}'\n",
    "\n",
    "        response = model.invoke(prompt)\n",
    "\n",
    "        memory.add_user_message(question)\n",
    "        memory.add_bot_message(response)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    chain = ConversationChain(memory = memory, conversation_model = rag_retrieval)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 (Self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatHistory = \"Conversation So Far : \\n\"\n",
    "\n",
    "template = \"\"\"\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from it's knowledge base or from the previous conversation depending on what the question is. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "\n",
    "Conversation History : \\n{conversation}\n",
    "\n",
    "AI's Knowledge base : {context}\n",
    "\n",
    "Human's Question : {question}\n",
    "\n",
    "AI : \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def chat(question): \n",
    "    \n",
    "    global chatHistory\n",
    "    \n",
    "    # Retrieve docs from vector DB\n",
    "    docs = vectors.similarity_search(question, k = 4)\n",
    "    contextString = combineDocs(docs)\n",
    "    \n",
    "    query = prompt.format(conversation = chatHistory, context = contextString, question = question)\n",
    "\n",
    "    response = model.invoke(query)\n",
    "    \n",
    "    chatHistory = chatHistory + \"\\nHuman's question : \" + question + \"\\nAI : \" + response\n",
    "\n",
    "    return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
