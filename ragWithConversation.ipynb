{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"llama3\"; \n",
    "Embedding_Model = \"mxbai-embed-large\"\n",
    "model = Ollama(model=Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path=\"testDataMD\",glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"##\", chunk_size=1000, chunk_overlap=0)\n",
    "docs2 = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = FAISS.from_documents(docs2, OllamaEmbeddings(model = Embedding_Model))\n",
    "retriever2 = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to combine retrieved documents into a string \n",
    "\n",
    "def combineDocs(docs):\n",
    "    context = \"\\n\\n\".join(f'Document [ Metadata : \\n{doc.metadata} \\nPage Content : \\n{doc.page_content} ]' for doc in docs)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vectorDB):\n",
    "    \n",
    "    # This is where the conversation will be stored\n",
    "    memory = ConversationBufferMemory()\n",
    "\n",
    "    # Creating a conversation model for ConvChain\n",
    "\n",
    "    def rag_retrieval(question, memory):\n",
    "\n",
    "        # Retrieve docs from vector DB\n",
    "        docs = vectorDB.similarity_search(question, k = 4)\n",
    "        context = combineDocs(docs)\n",
    "\n",
    "        conversation = memory.load_memory_variables({}).get('history', '')\n",
    "\n",
    "        # Add context to the prompt\n",
    "        prompt = f'Answer the question based on the context below and the previous conversations. If you cannot\\nanswer the question, reply \"Oof that\\'s a tough one, i don\\'t really know this\"\\n\\nPrevious Conversation : \\n{conversation}\\n\\nContext : \\n{context}\\n\\nQuestion from user: {question}'\n",
    "\n",
    "        response = model.invoke(prompt)\n",
    "\n",
    "        memory.add_user_message(question)\n",
    "        memory.add_bot_message(response)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    chain = ConversationChain(memory = memory, conversation_model = rag_retrieval)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatHistory = \"\\n\\n\"\n",
    "\n",
    "template = \"\"\"\n",
    "You are acting as a chat-bot\n",
    "Answer the user's question based on the context (provided by user as Documents) below and the previous conversations between you (AI) and the user (User). \n",
    "Utilise your understanding to see whether context is to be used or the previous conversation is to be used or both together to best answer the question\n",
    "If you can't answer the question, reply \"Oof that's a tough one, i don't really know this\"\n",
    "\n",
    "Conversation till now : {conversation}\n",
    "\n",
    "Context : {context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def chat(question): \n",
    "    \n",
    "    # Retrieve docs from vector DB\n",
    "    docs = vectors.similarity_search(question, k = 4)\n",
    "    contextString = combineDocs(docs)\n",
    "    \n",
    "    query = prompt.format(conversation = chatHistory, context = contextString, question = question)\n",
    "\n",
    "    response = model.invoke(query)\n",
    "    \n",
    "    chatHistory = chatHistory + \"\\nUser : \" + question + \"\\nAI : \" + response\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
