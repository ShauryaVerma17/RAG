{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"llama3\"; \n",
    "model = Ollama(model=Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"The following is a conversation between a Human and and you (an AI Assistant). \n",
    "The Human will give a few documents as context and also his query.\n",
    "You are always straight to the point and use only the details from the context given by the user and the conversation history. \n",
    "If you don't the answer to a question based on the previous conversation, you can say does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "inputmessage = \"\"\n",
    "context = \"\"\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=model,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"testDataPDF\",glob=\"*.pdf\",loader_cls=PyMuPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, OllamaEmbeddings(model = \"mxbai-embed-large\"))\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, docs):\n",
    "\n",
    "    context = \"\\n\\n\".join(f'Document [ Metadata : \\n{doc.metadata} \\nPage Content : \\n{doc.page_content} ]' for doc in docs)\n",
    "    combined = f\"Here's some Context : \\n {context} \\n\\n Here's my question : {input}\"\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question) : \n",
    "    docs = retriever.invoke(question)\n",
    "    query = combine(question, docs)\n",
    "    return conversation.predict(input = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"What is the browser's high level structure?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
